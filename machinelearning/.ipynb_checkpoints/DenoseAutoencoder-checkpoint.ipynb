{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luokui/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/luokui/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/Users/luokui/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/luokui/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=2))\n",
    "\n",
    "x = tf.constant([[0.7, 0.1]])\n",
    "a = tf.matmul(x, w1)\n",
    "b = tf.matmul(a, w2)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) # 注意这里的写法\n",
    "    print(sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test TF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构造一个简单的数据集。\n",
    "train_x = np.r_[np.random.randn(10000, 2), np.random.randn(10000, 2) + 2]\n",
    "train_y = np.r_[np.repeat(-1, 10000), np.repeat(1, 10000)].reshape(-1, 1)\n",
    "\n",
    "def generate_data(x, y, size=50):\n",
    "    alls = y.shape[0]\n",
    "    n = int(alls / size)\n",
    "    for i in range(n-1):\n",
    "        yield x[size * i : size * (i+1)] , y[size * i : size * (i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses is 5.7107648849487305\n",
      "losses is 5.575189113616943\n",
      "[[-0.6642167   1.6406729  -0.1005234 ]\n",
      " [-2.7850468  -0.27976224  0.9921207 ]]\n",
      "[[-0.6642167   1.6406729  -0.1005234 ]\n",
      " [-2.7850468  -0.27976224  0.9921207 ]]\n"
     ]
    }
   ],
   "source": [
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=2))\n",
    "x = tf.placeholder(tf.float32, shape=[None, 2], name=\"input_x\")\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 1], name=\"out_y\")\n",
    "model_1 = tf.matmul(x, w1)\n",
    "model_2 = tf.matmul(model_1, w2)\n",
    "loss = - tf.reduce_mean(-y_ * tf.log(tf.clip_by_value(model_2, 1e-10, 1)))\n",
    "train_op = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(10):\n",
    "        for d_x, d_y in generate_data(train_x, train_y):\n",
    "            sess.run(train_op, feed_dict={x: d_x, y_: d_y})\n",
    "        if i % 5==0:\n",
    "            losses = sess.run(loss, feed_dict={x:train_x, y_: train_y})\n",
    "            print(\"losses is {}\".format(losses))\n",
    "    print(sess.run(w1))\n",
    "    print(sess.run(w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack denoisingEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter0 , losses is 2.455381393432617\n",
      "iter10 , losses is 2.09230375289917\n",
      "iter20 , losses is 2.0386877059936523\n",
      "iter30 , losses is 2.0552096366882324\n",
      "iter40 , losses is 2.0176775455474854\n"
     ]
    }
   ],
   "source": [
    "train_x = np.r_[np.random.randn(10000, 5), np.random.randn(10000, 5) + 2]\n",
    "\n",
    "def gen_data(x, size=50):\n",
    "    alls = x.shape[0]\n",
    "    n = int(alls / size)\n",
    "    for i in range(n-1):\n",
    "        yield x[size * i : size * (i+1)]\n",
    "\n",
    "class denoise_auto:\n",
    "    def __init__(self, hidden_size, input_shape):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, self.input_shape], name=\"input_x\")\n",
    "        self.mask = tf.placeholder(tf.float32, shape=[None, self.input_shape], name=\"mask_s\")\n",
    "        \n",
    "        w_init_max = 4 * np.sqrt(6./ (self.hidden_size + self.input_shape))\n",
    "        self.w1 = tf.Variable(tf.random_uniform(shape=[self.input_shape, self.hidden_size], minval=-w_init_max,\n",
    "                                  maxval=w_init_max), name=\"input_w1\")\n",
    "        self.b1 = tf.Variable(tf.zeros([self.hidden_size]), name=\"b1\")\n",
    "        self._w1_t = tf.transpose(self.w1)\n",
    "        self._b1_t = tf.Variable(tf.zeros([self.input_shape]), name=\"b1_t\")\n",
    "        \n",
    "        tild_x = tf.multiply(self.x, self.mask)\n",
    "        # 这里有主要 如何 使用masking\n",
    "        self.y = tf.nn.relu(tf.matmul(tild_x, self.w1) + self.b1)\n",
    "        z = tf.nn.sigmoid(tf.matmul(self.y, self._w1_t) + self._b1_t)\n",
    "        self.cost = tf.reduce_mean(tf.pow(z - self.x, 2))\n",
    "        self.train_op = tf.train.GradientDescentOptimizer(0.01).minimize(self.cost)\n",
    "        \n",
    "        \n",
    "def fitdatas(train_x, hidden_size, mask_ratio=0.8, epochs=50, batch_size=200):\n",
    "    input_shape = train_x.shape[1]\n",
    "    with tf.Session() as sess:\n",
    "        model = denoise_auto(hidden_size=hidden_size, input_shape=input_shape)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(epochs):\n",
    "            for d_x in gen_data(train_x, size=batch_size):\n",
    "                mask_vale = np.random.binomial(1, mask_ratio, size=d_x.shape)\n",
    "                feed_dicts = {model.x: d_x, model.mask: mask_vale}\n",
    "                _, losses = sess.run([model.train_op, model.cost], feed_dict=feed_dicts)\n",
    "            if i % 10 == 0:\n",
    "                print(\"iter{} , losses is {}\".format(i, losses))      \n",
    "        mask_encoder = np.random.binomial(1, 1, size=train_x.shape)\n",
    "        encoder_x, w1, b1 = sess.run([model.y, model.w1, model.b1], feed_dict={model.x: train_x, model.mask: mask_encoder})\n",
    "    return w1, b1, encoder_x\n",
    "    \n",
    "#w_1, b_1, encoders = fitdatas(train_x, hidden_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# 构造一个简单的数据集。\n",
    "train_x = np.r_[np.random.randn(10000, 5), np.random.randn(10000, 5) + 2, np.random.randn(10000, 5) + 3]\n",
    "train_y = np.r_[np.repeat(0, 10000), np.repeat(1, 10000), np.repeat(2, 10000)].reshape(-1, 1)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# binarizer 也是一个有用的地方。\n",
    "#ohot = OneHotEncoder()\n",
    "#train_y_onehot = ohot.fit_transform(train_y).toarray()\n",
    "train_y_onehot = OneHotEncoder().fit_transform(train_y).toarray()\n",
    "\n",
    "def generate_data(x, y, size=100):\n",
    "    alls = y.shape[0]\n",
    "    n = int(alls / size)\n",
    "    for i in range(n-1):\n",
    "        yield x[size * i : size * (i+1)] , y[size * i : size * (i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter0 , losses is 5.606483459472656\n",
      "iter0 , losses is 84.56307983398438\n",
      "iter0 , losses is 895.0272216796875\n"
     ]
    }
   ],
   "source": [
    "lists = [10, 10, 10]\n",
    "auto_outs = []\n",
    "w_outs = []\n",
    "b_outs = []\n",
    "fit_data = train_x\n",
    "for index, size in enumerate(lists):\n",
    "    w, b, encoders =  fitdatas(fit_data, hidden_size=size, epochs=10)\n",
    "    auto_outs.append(encoders)\n",
    "    w_outs.append(w)\n",
    "    b_outs.append(b)\n",
    "    fit_data = encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses is 0.6378161311149597, acc is 0.3333333432674408\n",
      "losses is 0.34858831763267517, acc is 0.7182666659355164\n",
      "losses is 0.31274452805519104, acc is 0.7983666658401489\n",
      "losses is 0.30125391483306885, acc is 0.8094666600227356\n",
      "losses is 0.2911825478076935, acc is 0.8147333264350891\n",
      "losses is 0.28002288937568665, acc is 0.8197000026702881\n",
      "losses is 0.2727765440940857, acc is 0.8237000107765198\n",
      "losses is 0.2838453948497772, acc is 0.8209666609764099\n",
      "losses is 0.28483638167381287, acc is 0.8266666531562805\n",
      "losses is 0.28455671668052673, acc is 0.8291333317756653\n",
      "losses is 0.2806958854198456, acc is 0.8327000141143799\n",
      "losses is 0.2758401930332184, acc is 0.8343999981880188\n",
      "losses is 0.2722272276878357, acc is 0.8353999853134155\n",
      "losses is 0.26942726969718933, acc is 0.8366333246231079\n",
      "losses is 0.2718792259693146, acc is 0.8375333547592163\n",
      "losses is 0.2703576385974884, acc is 0.8382666707038879\n",
      "losses is 0.2679765522480011, acc is 0.8400333523750305\n",
      "losses is 0.26488324999809265, acc is 0.8421666622161865\n",
      "losses is 0.26108768582344055, acc is 0.8447333574295044\n",
      "losses is 0.257233202457428, acc is 0.8471666574478149\n",
      "losses is 0.2539222538471222, acc is 0.8494666814804077\n",
      "losses is 0.25108301639556885, acc is 0.8514333367347717\n",
      "losses is 0.24829518795013428, acc is 0.8531333208084106\n",
      "losses is 0.24546141922473907, acc is 0.8551333546638489\n",
      "losses is 0.24282033741474152, acc is 0.8562666773796082\n",
      "losses is 0.2404812127351761, acc is 0.8568999767303467\n",
      "losses is 0.23846113681793213, acc is 0.8579333424568176\n",
      "losses is 0.23666661977767944, acc is 0.8582666516304016\n",
      "losses is 0.23497000336647034, acc is 0.8586999773979187\n",
      "losses is 0.23334600031375885, acc is 0.8593666553497314\n",
      "losses is 0.231758251786232, acc is 0.8602333068847656\n",
      "losses is 0.23022526502609253, acc is 0.8612666726112366\n",
      "losses is 0.22890876233577728, acc is 0.8617333173751831\n",
      "losses is 0.22774986922740936, acc is 0.8624666929244995\n",
      "losses is 0.22666046023368835, acc is 0.8630333542823792\n",
      "losses is 0.22562755644321442, acc is 0.8633666634559631\n",
      "losses is 0.22464878857135773, acc is 0.8639666438102722\n",
      "losses is 0.22372469305992126, acc is 0.8645333051681519\n",
      "losses is 0.2228604555130005, acc is 0.8651999831199646\n",
      "losses is 0.22207647562026978, acc is 0.8656333088874817\n",
      "losses is 0.22142821550369263, acc is 0.8660666942596436\n",
      "losses is 0.22075200080871582, acc is 0.8664666414260864\n",
      "losses is 0.21996447443962097, acc is 0.8668000102043152\n",
      "losses is 0.2191416472196579, acc is 0.8671666383743286\n",
      "losses is 0.2182893455028534, acc is 0.8675000071525574\n",
      "losses is 0.21749956905841827, acc is 0.8676999807357788\n",
      "losses is 0.21679288148880005, acc is 0.867733359336853\n",
      "losses is 0.21612194180488586, acc is 0.8678666949272156\n",
      "losses is 0.2154729962348938, acc is 0.8680999875068665\n",
      "losses is 0.21484418213367462, acc is 0.8683666586875916\n"
     ]
    }
   ],
   "source": [
    "input_shape = 5\n",
    "y_shape = 3\n",
    "\n",
    "x_stack = tf.placeholder(tf.float32, shape=[None, input_shape], name=\"stack_x\")\n",
    "y_stack = tf.placeholder(tf.float32, shape=[None, y_shape], name=\"stack_y\")\n",
    "\n",
    "fit_stack = x_stack\n",
    "for index in range(len(lists)):\n",
    "    temp_w = tf.Variable(w_outs[index], name=\"lay_w{}\".format(index))\n",
    "    temp_b = tf.Variable(b_outs[index], name=\"lay_b{}\".format(index))\n",
    "    #temp_input = tf.\n",
    "    fit_stack = tf.nn.sigmoid(tf.nn.xw_plus_b(fit_stack, temp_w, temp_b),\n",
    "                              name=\"encod_{}\".format(index))\n",
    "    \n",
    "\n",
    "last_w = tf.Variable(tf.truncated_normal([fit_stack.get_shape()[1].value, y_shape], stddev=0.1), name=\"last_w\")\n",
    "last_b = tf.Variable(tf.constant(0.1, shape=[y_shape]), name=\"last_b\")\n",
    "y_ = tf.nn.xw_plus_b(fit_stack, last_w, last_b)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_, labels=y_stack))\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss=cost)\n",
    "\n",
    "model_pred = tf.argmax(y_, 1)\n",
    "correct_pred = tf.equal(model_pred, tf.argmax(y_stack, 1))\n",
    "acc = tf.reduce_mean(tf.cast(correct_pred, \"float\"))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1000):\n",
    "        for d_x, d_y in generate_data(train_x, train_y_onehot):\n",
    "            sess.run(train_op, feed_dict={x_stack: d_x, y_stack: d_y})\n",
    "        if i % 20 == 0:\n",
    "            losses, correct_preds, accuracy = sess.run([cost, correct_pred, acc], feed_dict={x_stack: train_x, y_stack: train_y_onehot})\n",
    "            print(\"losses is {}, acc is {}\".format(losses, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# denoise AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 10\n",
    "input_shape = 5\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, input_shape], name=\"input_x\")\n",
    "mask = tf.placeholder(tf.float32, shape=[None, input_shape], name=\"mask_s\")\n",
    "\n",
    "w_init_max = 4 * np.sqrt(6./ (hidden_size + input_shape))\n",
    "w1 = tf.Variable(tf.random_uniform(shape=[input_shape, hidden_size], minval=-w_init_max,\n",
    "                                  maxval=w_init_max), name=\"input_w1\")\n",
    "b1 = tf.Variable(tf.zeros([hidden_size]), name=\"b1\")\n",
    "\n",
    "w1_t = tf.transpose(w1)\n",
    "b1_t = tf.Variable(tf.zeros([input_shape]), name=\"b1_t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tild_x = tf.multiply(x, mask)\n",
    "# 这里有主要 如何 使用masking\n",
    "y = tf.nn.relu(tf.matmul(tild_x, w1) + b1)\n",
    "z = tf.nn.sigmoid(tf.matmul(y, w1_t) + b1_t)\n",
    "cost = tf.reduce_mean(tf.pow(z - x, 2))\n",
    "train_op = tf.train.GradientDescentOptimizer(0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses is 1.8198922872543335\n",
      "losses is 1.4293465614318848\n",
      "losses is 1.4079359769821167\n",
      "losses is 1.3945034742355347\n",
      "losses is 1.3875428438186646\n",
      "losses is 1.3836427927017212\n",
      "losses is 1.3799068927764893\n",
      "losses is 1.3764300346374512\n",
      "losses is 1.3767027854919434\n",
      "losses is 1.374959111213684\n",
      "[[-1.9464875  -1.109545    1.4265246   0.8759942  -2.2511616   1.343049\n",
      "   0.23419555 -2.150114   -0.7549587   2.4260287 ]\n",
      " [-1.426449    1.1320349  -0.39795464 -1.8483213   0.83186305 -0.10572219\n",
      "   1.0274603  -1.8152014  -0.9173398   0.99109113]\n",
      " [ 0.7949902  -1.8315792   0.31412092 -1.926579   -0.31639603 -1.1137276\n",
      "   1.6101949  -0.99653935  1.1029608   0.79431856]\n",
      " [ 1.1451286  -1.9548414  -1.7246299  -0.05418967 -0.4079116  -0.22205326\n",
      "   0.7431589  -0.15360783 -2.3336692   1.3408062 ]\n",
      " [-1.3907019  -0.99031746  0.09212247 -1.2605349  -0.1283643   1.6878622\n",
      "   2.027463    1.3425846  -1.3327801  -0.20025294]]\n",
      "[[-1.9464875  -1.426449    0.7949902   1.1451286  -1.3907019 ]\n",
      " [-1.109545    1.1320349  -1.8315792  -1.9548414  -0.99031746]\n",
      " [ 1.4265246  -0.39795464  0.31412092 -1.7246299   0.09212247]\n",
      " [ 0.8759942  -1.8483213  -1.926579   -0.05418967 -1.2605349 ]\n",
      " [-2.2511616   0.83186305 -0.31639603 -0.4079116  -0.1283643 ]\n",
      " [ 1.343049   -0.10572219 -1.1137276  -0.22205326  1.6878622 ]\n",
      " [ 0.23419555  1.0274603   1.6101949   0.7431589   2.027463  ]\n",
      " [-2.150114   -1.8152014  -0.99653935 -0.15360783  1.3425846 ]\n",
      " [-0.7549587  -0.9173398   1.1029608  -2.3336692  -1.3327801 ]\n",
      " [ 2.4260287   0.99109113  0.79431856  1.3408062  -0.20025294]]\n",
      "[[ 0.31974345  5.636023    0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 1.9731084   3.9346929   0.         ...  1.1725143   0.9508567\n",
      "   0.        ]\n",
      " [ 0.31225693  0.          0.04772845 ...  1.1924071   0.08836883\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "  10.755152  ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "  10.027711  ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "  10.940053  ]]\n"
     ]
    }
   ],
   "source": [
    "train_x = np.r_[np.random.randn(10000, 5), np.random.randn(10000, 5) + 2]\n",
    "# train_y = np.r_[np.repeat(-1, 10000), np.repeat(1, 10000)].reshape(-1, 1)\n",
    "\n",
    "def gen_data(x, size=50):\n",
    "    alls = x.shape[0]\n",
    "    n = int(alls / size)\n",
    "    for i in range(n-1):\n",
    "        yield x[size * i : size * (i+1)]\n",
    "        \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(50):\n",
    "        for d_x in gen_data(train_x):\n",
    "            mask_vale = np.random.binomial(1, 0.8, size=d_x.shape)\n",
    "            sess.run(train_op, feed_dict={x: d_x, mask: mask_vale})\n",
    "        if i % 5==0:\n",
    "            mask_vale = np.random.binomial(1, 0.8, size=train_x.shape)\n",
    "            losses = sess.run(cost, feed_dict={x:train_x, mask: mask_vale})\n",
    "            print(\"losses is {}\".format(losses))\n",
    "    print(sess.run(w1))\n",
    "    print(sess.run(w1_t))\n",
    "    mask_encoder = np.random.binomial(1, 1, size=train_x.shape)\n",
    "    #encoder_x = sess.run(y, feed_dict={x: train_x, mask: mask_encoder})\n",
    "    encoder_x = y.eval({x: train_x, mask: mask_encoder})\n",
    "    print(encoder_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses is 1.5699390172958374\n",
      "losses is 1.4798840284347534\n",
      "[[-0.03909982  0.78273857  1.8065187  -0.48890954  1.9585708   0.12119851\n",
      "  -2.050288   -0.18503776 -1.1932139   0.55353874]\n",
      " [ 2.2645807  -0.30339217  1.4423703  -0.5439672  -1.3680559   1.2672769\n",
      "  -0.9956454  -0.80652535  1.4804232  -2.2291338 ]\n",
      " [-0.26816654 -1.4777379   2.1162074   1.3133695   0.40296686 -2.2469344\n",
      "   0.37932917 -0.86401117  1.1084111   1.1243018 ]\n",
      " [ 0.2811688  -1.7740858   1.2804466  -1.2298981   0.8681259  -0.2088778\n",
      "  -0.95917493  2.3373055  -0.14740819 -0.6989715 ]\n",
      " [ 2.109717   -0.6849237   0.71669406  1.828332   -1.3892382  -0.2631936\n",
      "  -2.0900462   0.89236265 -0.14709139  0.18654285]]\n",
      "[[ 0.          0.          0.         ...  0.10422404  1.2223064\n",
      "   1.3990352 ]\n",
      " [ 1.3104633   0.          5.4927497  ...  0.          2.2232661\n",
      "   0.89440036]\n",
      " [ 1.2108825   0.          0.         ...  2.0686102   1.0236765\n",
      "   0.        ]\n",
      " ...\n",
      " [ 7.8629255   0.          8.645921   ...  8.125151    0.\n",
      "   2.5048468 ]\n",
      " [11.039127    0.         16.44126    ...  8.736963    0.\n",
      "   0.        ]\n",
      " [ 5.520565    0.         13.689323   ...  0.          0.86514276\n",
      "   0.25267494]]\n"
     ]
    }
   ],
   "source": [
    "class denoise_autoencoder:\n",
    "    def __init__(self, hidden_size, input_shape):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, self.input_shape], name=\"input_x\")\n",
    "        self.mask = tf.placeholder(tf.float32, shape=[None, self.input_shape], name=\"mask_s\")\n",
    "        \n",
    "        w_init_max = 4 * np.sqrt(6./ (self.hidden_size + self.input_shape))\n",
    "        self.w1 = tf.Variable(tf.random_uniform(shape=[self.input_shape, self.hidden_size], minval=-w_init_max,\n",
    "                                  maxval=w_init_max), name=\"input_w1\")\n",
    "        self._b1 = tf.Variable(tf.zeros([self.hidden_size]), name=\"b1\")\n",
    "        self._w1_t = tf.transpose(self.w1)\n",
    "        self._b1_t = tf.Variable(tf.zeros([self.input_shape]), name=\"b1_t\")\n",
    "        self.cost, self.train_op, self.encoders = self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        tild_x = tf.multiply(self.x, self.mask)\n",
    "        # 这里有主要 如何 使用masking\n",
    "        y = tf.nn.relu(tf.matmul(tild_x, self.w1) + self._b1)\n",
    "        z = tf.nn.sigmoid(tf.matmul(y, self._w1_t) + self._b1_t)\n",
    "        cost = tf.reduce_mean(tf.pow(z - self.x, 2))\n",
    "        train_op = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "        return cost, train_op, y\n",
    "\n",
    "            \n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "    allow_soft_placement=True,\n",
    "    log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        model = denoise_autoencoder(hidden_size=10, input_shape=5)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(10):\n",
    "            for d_x in gen_data(train_x):\n",
    "                mask_value = np.random.binomial(1, 0.8, size=d_x.shape)\n",
    "                sess.run(model.train_op, feed_dict={model.x: d_x, model.mask: mask_value})\n",
    "            if i % 5==0:\n",
    "                mask_vale = np.random.binomial(1, 0.8, size=train_x.shape)\n",
    "                losses = sess.run(model.cost, feed_dict={model.x:train_x, model.mask: mask_vale})\n",
    "                print(\"losses is {}\".format(losses))\n",
    "        print(sess.run(model.w1))\n",
    "        # print(sess.run(model.w1_t))\n",
    "        mask_encoder = np.random.binomial(1, 1, size=train_x.shape)\n",
    "        encoder_x = sess.run(model.encoders, feed_dict={model.x: train_x, model.mask: mask_encoder})\n",
    "        #encoder_x = y.eval({x: train_x, mask: mask_encoder})\n",
    "        print(encoder_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 10 表示 进行10次试验， 成功的概率为 0.2, 进行分析的\n",
    "# np.random.binomial(10, 0.2, size=(10, 2))\n",
    "# np.random.binomial(1, 1, size=(10, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
