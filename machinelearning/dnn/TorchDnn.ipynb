{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = 20\n",
    "n1 = 10\n",
    "n2 = 5\n",
    "\n",
    "d1 = np.random.randn(m, n1)\n",
    "y = np.random.randn(m, 1)\n",
    "\n",
    "w1 = np.random.randn(n1, n2)\n",
    "w2 = np.random.randn(n2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.96075754, 0.        , 0.        , 0.        ],\n",
       "       [0.38622568, 1.29359931, 1.25797654, 0.57996264, 0.        ],\n",
       "       [0.        , 0.        , 0.61987001, 0.        , 1.14217301],\n",
       "       [0.        , 0.9336378 , 0.        , 0.47492992, 0.        ],\n",
       "       [0.        , 0.19199572, 0.        , 0.        , 0.        ],\n",
       "       [1.23298755, 0.        , 0.        , 1.44978411, 0.38110767],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.12658378],\n",
       "       [0.        , 0.        , 0.32767962, 1.00366672, 0.        ],\n",
       "       [1.30810357, 0.        , 0.        , 0.7352494 , 0.63978023],\n",
       "       [1.0459267 , 0.30260496, 0.        , 1.91188523, 0.0238319 ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.maximum(w1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iters is 20, loss is 1.3657465919581884\n",
      " iters is 40, loss is 1.0972875151657984\n",
      " iters is 60, loss is 0.9725905244612427\n",
      " iters is 80, loss is 0.9065155904131714\n",
      " iters is 100, loss is 0.8593031846088199\n"
     ]
    }
   ],
   "source": [
    "iters = 0\n",
    "while iters < 100:\n",
    "    iters += 1\n",
    "    y1 = d1.dot(w1)\n",
    "    y1_relu = np.maximum(y1, 0)\n",
    "    y2 = y1_relu.dot(w2)\n",
    "    loss = 1 / 20 * np.power(y2 - y, 2).sum()\n",
    "    \n",
    "    if iters % 20 == 0:\n",
    "        print(\" iters is {}, loss is {}\".format(iters, loss))\n",
    "    \n",
    "    error = 2 * (y2 - y)\n",
    "    grad_w2 = y1_relu.T.dot(error)\n",
    "    \n",
    "    temp_error = error.dot(w2.T)\n",
    "    \n",
    "    temp_grad = temp_error.copy()\n",
    "    temp_grad[y1<0] = 0\n",
    "    grad_w1 = d1.T.dot(temp_grad)\n",
    "    \n",
    "    w1 -= 0.001 * grad_w1\n",
    "    w2 -= 0.001 * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 27372644.0\n",
      "1 991212732416.0\n",
      "2 7.761842844963043e+24\n",
      "3 inf\n",
      "4 nan\n",
      "5 nan\n",
      "6 nan\n",
      "7 nan\n",
      "8 nan\n",
      "9 nan\n",
      "10 nan\n",
      "11 nan\n",
      "12 nan\n",
      "13 nan\n",
      "14 nan\n",
      "15 nan\n",
      "16 nan\n",
      "17 nan\n",
      "18 nan\n",
      "19 nan\n",
      "20 nan\n",
      "21 nan\n",
      "22 nan\n",
      "23 nan\n",
      "24 nan\n",
      "25 nan\n",
      "26 nan\n",
      "27 nan\n",
      "28 nan\n",
      "29 nan\n",
      "30 nan\n",
      "31 nan\n",
      "32 nan\n",
      "33 nan\n",
      "34 nan\n",
      "35 nan\n",
      "36 nan\n",
      "37 nan\n",
      "38 nan\n",
      "39 nan\n",
      "40 nan\n",
      "41 nan\n",
      "42 nan\n",
      "43 nan\n",
      "44 nan\n",
      "45 nan\n",
      "46 nan\n",
      "47 nan\n",
      "48 nan\n",
      "49 nan\n",
      "50 nan\n",
      "51 nan\n",
      "52 nan\n",
      "53 nan\n",
      "54 nan\n",
      "55 nan\n",
      "56 nan\n",
      "57 nan\n",
      "58 nan\n",
      "59 nan\n",
      "60 nan\n",
      "61 nan\n",
      "62 nan\n",
      "63 nan\n",
      "64 nan\n",
      "65 nan\n",
      "66 nan\n",
      "67 nan\n",
      "68 nan\n",
      "69 nan\n",
      "70 nan\n",
      "71 nan\n",
      "72 nan\n",
      "73 nan\n",
      "74 nan\n",
      "75 nan\n",
      "76 nan\n",
      "77 nan\n",
      "78 nan\n",
      "79 nan\n",
      "80 nan\n",
      "81 nan\n",
      "82 nan\n",
      "83 nan\n",
      "84 nan\n",
      "85 nan\n",
      "86 nan\n",
      "87 nan\n",
      "88 nan\n",
      "89 nan\n",
      "90 nan\n",
      "91 nan\n",
      "92 nan\n",
      "93 nan\n",
      "94 nan\n",
      "95 nan\n",
      "96 nan\n",
      "97 nan\n",
      "98 nan\n",
      "99 nan\n",
      "100 nan\n",
      "101 nan\n",
      "102 nan\n",
      "103 nan\n",
      "104 nan\n",
      "105 nan\n",
      "106 nan\n",
      "107 nan\n",
      "108 nan\n",
      "109 nan\n",
      "110 nan\n",
      "111 nan\n",
      "112 nan\n",
      "113 nan\n",
      "114 nan\n",
      "115 nan\n",
      "116 nan\n",
      "117 nan\n",
      "118 nan\n",
      "119 nan\n",
      "120 nan\n",
      "121 nan\n",
      "122 nan\n",
      "123 nan\n",
      "124 nan\n",
      "125 nan\n",
      "126 nan\n",
      "127 nan\n",
      "128 nan\n",
      "129 nan\n",
      "130 nan\n",
      "131 nan\n",
      "132 nan\n",
      "133 nan\n",
      "134 nan\n",
      "135 nan\n",
      "136 nan\n",
      "137 nan\n",
      "138 nan\n",
      "139 nan\n",
      "140 nan\n",
      "141 nan\n",
      "142 nan\n",
      "143 nan\n",
      "144 nan\n",
      "145 nan\n",
      "146 nan\n",
      "147 nan\n",
      "148 nan\n",
      "149 nan\n",
      "150 nan\n",
      "151 nan\n",
      "152 nan\n",
      "153 nan\n",
      "154 nan\n",
      "155 nan\n",
      "156 nan\n",
      "157 nan\n",
      "158 nan\n",
      "159 nan\n",
      "160 nan\n",
      "161 nan\n",
      "162 nan\n",
      "163 nan\n",
      "164 nan\n",
      "165 nan\n",
      "166 nan\n",
      "167 nan\n",
      "168 nan\n",
      "169 nan\n",
      "170 nan\n",
      "171 nan\n",
      "172 nan\n",
      "173 nan\n",
      "174 nan\n",
      "175 nan\n",
      "176 nan\n",
      "177 nan\n",
      "178 nan\n",
      "179 nan\n",
      "180 nan\n",
      "181 nan\n",
      "182 nan\n",
      "183 nan\n",
      "184 nan\n",
      "185 nan\n",
      "186 nan\n",
      "187 nan\n",
      "188 nan\n",
      "189 nan\n",
      "190 nan\n",
      "191 nan\n",
      "192 nan\n",
      "193 nan\n",
      "194 nan\n",
      "195 nan\n",
      "196 nan\n",
      "197 nan\n",
      "198 nan\n",
      "199 nan\n",
      "200 nan\n",
      "201 nan\n",
      "202 nan\n",
      "203 nan\n",
      "204 nan\n",
      "205 nan\n",
      "206 nan\n",
      "207 nan\n",
      "208 nan\n",
      "209 nan\n",
      "210 nan\n",
      "211 nan\n",
      "212 nan\n",
      "213 nan\n",
      "214 nan\n",
      "215 nan\n",
      "216 nan\n",
      "217 nan\n",
      "218 nan\n",
      "219 nan\n",
      "220 nan\n",
      "221 nan\n",
      "222 nan\n",
      "223 nan\n",
      "224 nan\n",
      "225 nan\n",
      "226 nan\n",
      "227 nan\n",
      "228 nan\n",
      "229 nan\n",
      "230 nan\n",
      "231 nan\n",
      "232 nan\n",
      "233 nan\n",
      "234 nan\n",
      "235 nan\n",
      "236 nan\n",
      "237 nan\n",
      "238 nan\n",
      "239 nan\n",
      "240 nan\n",
      "241 nan\n",
      "242 nan\n",
      "243 nan\n",
      "244 nan\n",
      "245 nan\n",
      "246 nan\n",
      "247 nan\n",
      "248 nan\n",
      "249 nan\n",
      "250 nan\n",
      "251 nan\n",
      "252 nan\n",
      "253 nan\n",
      "254 nan\n",
      "255 nan\n",
      "256 nan\n",
      "257 nan\n",
      "258 nan\n",
      "259 nan\n",
      "260 nan\n",
      "261 nan\n",
      "262 nan\n",
      "263 nan\n",
      "264 nan\n",
      "265 nan\n",
      "266 nan\n",
      "267 nan\n",
      "268 nan\n",
      "269 nan\n",
      "270 nan\n",
      "271 nan\n",
      "272 nan\n",
      "273 nan\n",
      "274 nan\n",
      "275 nan\n",
      "276 nan\n",
      "277 nan\n",
      "278 nan\n",
      "279 nan\n",
      "280 nan\n",
      "281 nan\n",
      "282 nan\n",
      "283 nan\n",
      "284 nan\n",
      "285 nan\n",
      "286 nan\n",
      "287 nan\n",
      "288 nan\n",
      "289 nan\n",
      "290 nan\n",
      "291 nan\n",
      "292 nan\n",
      "293 nan\n",
      "294 nan\n",
      "295 nan\n",
      "296 nan\n",
      "297 nan\n",
      "298 nan\n",
      "299 nan\n",
      "300 nan\n",
      "301 nan\n",
      "302 nan\n",
      "303 nan\n",
      "304 nan\n",
      "305 nan\n",
      "306 nan\n",
      "307 nan\n",
      "308 nan\n",
      "309 nan\n",
      "310 nan\n",
      "311 nan\n",
      "312 nan\n",
      "313 nan\n",
      "314 nan\n",
      "315 nan\n",
      "316 nan\n",
      "317 nan\n",
      "318 nan\n",
      "319 nan\n",
      "320 nan\n",
      "321 nan\n",
      "322 nan\n",
      "323 nan\n",
      "324 nan\n",
      "325 nan\n",
      "326 nan\n",
      "327 nan\n",
      "328 nan\n",
      "329 nan\n",
      "330 nan\n",
      "331 nan\n",
      "332 nan\n",
      "333 nan\n",
      "334 nan\n",
      "335 nan\n",
      "336 nan\n",
      "337 nan\n",
      "338 nan\n",
      "339 nan\n",
      "340 nan\n",
      "341 nan\n",
      "342 nan\n",
      "343 nan\n",
      "344 nan\n",
      "345 nan\n",
      "346 nan\n",
      "347 nan\n",
      "348 nan\n",
      "349 nan\n",
      "350 nan\n",
      "351 nan\n",
      "352 nan\n",
      "353 nan\n",
      "354 nan\n",
      "355 nan\n",
      "356 nan\n",
      "357 nan\n",
      "358 nan\n",
      "359 nan\n",
      "360 nan\n",
      "361 nan\n",
      "362 nan\n",
      "363 nan\n",
      "364 nan\n",
      "365 nan\n",
      "366 nan\n",
      "367 nan\n",
      "368 nan\n",
      "369 nan\n",
      "370 nan\n",
      "371 nan\n",
      "372 nan\n",
      "373 nan\n",
      "374 nan\n",
      "375 nan\n",
      "376 nan\n",
      "377 nan\n",
      "378 nan\n",
      "379 nan\n",
      "380 nan\n",
      "381 nan\n",
      "382 nan\n",
      "383 nan\n",
      "384 nan\n",
      "385 nan\n",
      "386 nan\n",
      "387 nan\n",
      "388 nan\n",
      "389 nan\n",
      "390 nan\n",
      "391 nan\n",
      "392 nan\n",
      "393 nan\n",
      "394 nan\n",
      "395 nan\n",
      "396 nan\n",
      "397 nan\n",
      "398 nan\n",
      "399 nan\n",
      "400 nan\n",
      "401 nan\n",
      "402 nan\n",
      "403 nan\n",
      "404 nan\n",
      "405 nan\n",
      "406 nan\n",
      "407 nan\n",
      "408 nan\n",
      "409 nan\n",
      "410 nan\n",
      "411 nan\n",
      "412 nan\n",
      "413 nan\n",
      "414 nan\n",
      "415 nan\n",
      "416 nan\n",
      "417 nan\n",
      "418 nan\n",
      "419 nan\n",
      "420 nan\n",
      "421 nan\n",
      "422 nan\n",
      "423 nan\n",
      "424 nan\n",
      "425 nan\n",
      "426 nan\n",
      "427 nan\n",
      "428 nan\n",
      "429 nan\n",
      "430 nan\n",
      "431 nan\n",
      "432 nan\n",
      "433 nan\n",
      "434 nan\n",
      "435 nan\n",
      "436 nan\n",
      "437 nan\n",
      "438 nan\n",
      "439 nan\n",
      "440 nan\n",
      "441 nan\n",
      "442 nan\n",
      "443 nan\n",
      "444 nan\n",
      "445 nan\n",
      "446 nan\n",
      "447 nan\n",
      "448 nan\n",
      "449 nan\n",
      "450 nan\n",
      "451 nan\n",
      "452 nan\n",
      "453 nan\n",
      "454 nan\n",
      "455 nan\n",
      "456 nan\n",
      "457 nan\n",
      "458 nan\n",
      "459 nan\n",
      "460 nan\n",
      "461 nan\n",
      "462 nan\n",
      "463 nan\n",
      "464 nan\n",
      "465 nan\n",
      "466 nan\n",
      "467 nan\n",
      "468 nan\n",
      "469 nan\n",
      "470 nan\n",
      "471 nan\n",
      "472 nan\n",
      "473 nan\n",
      "474 nan\n",
      "475 nan\n",
      "476 nan\n",
      "477 nan\n",
      "478 nan\n",
      "479 nan\n",
      "480 nan\n",
      "481 nan\n",
      "482 nan\n",
      "483 nan\n",
      "484 nan\n",
      "485 nan\n",
      "486 nan\n",
      "487 nan\n",
      "488 nan\n",
      "489 nan\n",
      "490 nan\n",
      "491 nan\n",
      "492 nan\n",
      "493 nan\n",
      "494 nan\n",
      "495 nan\n",
      "496 nan\n",
      "497 nan\n",
      "498 nan\n",
      "499 nan\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(i, loss.item())\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luokui/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/luokui/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/Users/luokui/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/luokui/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40709076.0\n",
      "38149920.0\n",
      "35984424.0\n",
      "28912360.0\n",
      "19015600.0\n",
      "10516210.0\n",
      "5591766.0\n",
      "3273796.5\n",
      "2070006.1\n",
      "1429997.8\n",
      "1059634.5\n",
      "827475.56\n",
      "669519.3\n",
      "554588.8\n",
      "466556.44\n",
      "396783.5\n",
      "340219.2\n",
      "293651.8\n",
      "254848.73\n",
      "222181.94\n",
      "194564.78\n",
      "171027.25\n",
      "150823.78\n",
      "133411.83\n",
      "118345.72\n",
      "105254.78\n",
      "93839.72\n",
      "83856.25\n",
      "75089.484\n",
      "67376.086\n",
      "60569.4\n",
      "54545.14\n",
      "49199.266\n",
      "44448.438\n",
      "40220.11\n",
      "36441.438\n",
      "33061.773\n",
      "30034.129\n",
      "27316.676\n",
      "24873.344\n",
      "22673.3\n",
      "20690.242\n",
      "18899.297\n",
      "17282.242\n",
      "15820.639\n",
      "14495.019\n",
      "13292.064\n",
      "12198.41\n",
      "11203.758\n",
      "10297.562\n",
      "9471.079\n",
      "8716.475\n",
      "8027.4253\n",
      "7397.308\n",
      "6821.131\n",
      "6293.2666\n",
      "5809.8726\n",
      "5366.413\n",
      "4959.218\n",
      "4585.2217\n",
      "4241.5273\n",
      "3926.1587\n",
      "3635.8962\n",
      "3368.522\n",
      "3122.2207\n",
      "2895.0574\n",
      "2685.515\n",
      "2492.1575\n",
      "2313.5078\n",
      "2148.5068\n",
      "1996.0181\n",
      "1854.9938\n",
      "1724.4664\n",
      "1603.6663\n",
      "1491.7485\n",
      "1388.0447\n",
      "1291.9226\n",
      "1202.7808\n",
      "1120.0928\n",
      "1043.3673\n",
      "972.1657\n",
      "906.03345\n",
      "844.6523\n",
      "787.58325\n",
      "734.54407\n",
      "685.2265\n",
      "639.37866\n",
      "596.73016\n",
      "557.02985\n",
      "520.096\n",
      "485.70462\n",
      "453.70193\n",
      "423.86346\n",
      "396.0644\n",
      "370.15765\n",
      "345.99414\n",
      "323.4759\n",
      "302.47443\n",
      "282.8802\n",
      "264.5992\n",
      "247.54306\n",
      "231.61278\n",
      "216.74406\n",
      "202.86313\n",
      "189.89902\n",
      "177.78351\n",
      "166.47083\n",
      "155.89322\n",
      "146.00876\n",
      "136.77396\n",
      "128.13437\n",
      "120.052605\n",
      "112.49695\n",
      "105.428406\n",
      "98.818306\n",
      "92.63265\n",
      "86.84114\n",
      "81.42141\n",
      "76.34963\n",
      "71.60246\n",
      "67.157715\n",
      "62.99288\n",
      "59.093994\n",
      "55.441097\n",
      "52.01828\n",
      "48.812202\n",
      "45.807934\n",
      "42.993202\n",
      "40.35422\n",
      "37.881233\n",
      "35.562347\n",
      "33.388187\n",
      "31.350368\n",
      "29.43832\n",
      "27.645067\n",
      "25.963785\n",
      "24.386444\n",
      "22.908222\n",
      "21.519524\n",
      "20.216465\n",
      "18.994436\n",
      "17.847322\n",
      "16.770668\n",
      "15.760893\n",
      "14.813559\n",
      "13.924433\n",
      "13.08967\n",
      "12.305494\n",
      "11.569163\n",
      "10.877792\n",
      "10.228075\n",
      "9.618099\n",
      "9.045068\n",
      "8.506058\n",
      "8.000781\n",
      "7.5250955\n",
      "7.078397\n",
      "6.6584272\n",
      "6.26418\n",
      "5.892887\n",
      "5.543935\n",
      "5.216546\n",
      "4.9085264\n",
      "4.618897\n",
      "4.346889\n",
      "4.090461\n",
      "3.8499267\n",
      "3.6233706\n",
      "3.4104548\n",
      "3.209916\n",
      "3.0218868\n",
      "2.8447833\n",
      "2.6780777\n",
      "2.5212593\n",
      "2.3736567\n",
      "2.2349918\n",
      "2.104299\n",
      "1.9815286\n",
      "1.8659813\n",
      "1.7570946\n",
      "1.6547403\n",
      "1.5584598\n",
      "1.4677675\n",
      "1.3821461\n",
      "1.3021157\n",
      "1.2264307\n",
      "1.155278\n",
      "1.0881308\n",
      "1.0252564\n",
      "0.9658878\n",
      "0.90992975\n",
      "0.8572317\n",
      "0.80756664\n",
      "0.7609789\n",
      "0.71706486\n",
      "0.6756599\n",
      "0.6366689\n",
      "0.60006267\n",
      "0.565437\n",
      "0.5328533\n",
      "0.5021342\n",
      "0.47327462\n",
      "0.44607013\n",
      "0.4203103\n",
      "0.39625823\n",
      "0.37355447\n",
      "0.35208344\n",
      "0.3318822\n",
      "0.31290823\n",
      "0.29493535\n",
      "0.2779984\n",
      "0.26207387\n",
      "0.24711668\n",
      "0.23301819\n",
      "0.2196837\n",
      "0.20712686\n",
      "0.19534677\n",
      "0.18415591\n",
      "0.17362165\n",
      "0.16374883\n",
      "0.15436035\n",
      "0.14561652\n",
      "0.1373217\n",
      "0.12947729\n",
      "0.122148335\n",
      "0.1151848\n",
      "0.10865188\n",
      "0.10244852\n",
      "0.096618086\n",
      "0.091128185\n",
      "0.08596192\n",
      "0.08108089\n",
      "0.0765172\n",
      "0.07216916\n",
      "0.06808951\n",
      "0.064229995\n",
      "0.060582865\n",
      "0.057151087\n",
      "0.05392365\n",
      "0.050895374\n",
      "0.048018567\n",
      "0.045286607\n",
      "0.04273866\n",
      "0.040311925\n",
      "0.038047545\n",
      "0.035901815\n",
      "0.033882175\n",
      "0.032007515\n",
      "0.03017793\n",
      "0.02848034\n",
      "0.026887722\n",
      "0.025378957\n",
      "0.023969784\n",
      "0.022625323\n",
      "0.021348111\n",
      "0.020144206\n",
      "0.01902856\n",
      "0.017976932\n",
      "0.016966784\n",
      "0.016034942\n",
      "0.015137879\n",
      "0.014296516\n",
      "0.013491977\n",
      "0.012752532\n",
      "0.012047462\n",
      "0.011386228\n",
      "0.010760473\n",
      "0.010174072\n",
      "0.009620788\n",
      "0.009092448\n",
      "0.008598461\n",
      "0.008126069\n",
      "0.0076825875\n",
      "0.0072653806\n",
      "0.0068636294\n",
      "0.006490068\n",
      "0.006135933\n",
      "0.005813155\n",
      "0.005498667\n",
      "0.0052066566\n",
      "0.0049255546\n",
      "0.0046698945\n",
      "0.004419374\n",
      "0.0041913446\n",
      "0.00397345\n",
      "0.0037670555\n",
      "0.0035686772\n",
      "0.0033846898\n",
      "0.0032037413\n",
      "0.0030407598\n",
      "0.0028871265\n",
      "0.002740685\n",
      "0.0026032808\n",
      "0.0024711594\n",
      "0.002353967\n",
      "0.0022358978\n",
      "0.0021262213\n",
      "0.0020230724\n",
      "0.0019232377\n",
      "0.0018301145\n",
      "0.0017406046\n",
      "0.0016581535\n",
      "0.0015856546\n",
      "0.0015109252\n",
      "0.0014424734\n",
      "0.0013752241\n",
      "0.0013165683\n",
      "0.0012526134\n",
      "0.0011957154\n",
      "0.0011448606\n",
      "0.0010931109\n",
      "0.0010470587\n",
      "0.0010007976\n",
      "0.0009595096\n",
      "0.0009197276\n",
      "0.00088064856\n",
      "0.0008432196\n",
      "0.0008088582\n",
      "0.00077745854\n",
      "0.0007430465\n",
      "0.00071339187\n",
      "0.0006852524\n",
      "0.0006579283\n",
      "0.0006326288\n",
      "0.0006085112\n",
      "0.0005848694\n",
      "0.0005616229\n",
      "0.0005406306\n",
      "0.0005199456\n",
      "0.0005022028\n",
      "0.00048488696\n",
      "0.00046669162\n",
      "0.0004496468\n",
      "0.00043284337\n",
      "0.00041782195\n",
      "0.00040261762\n",
      "0.00038875558\n",
      "0.0003751074\n",
      "0.00036314654\n",
      "0.00035124237\n",
      "0.000339427\n",
      "0.00032811772\n",
      "0.0003170952\n",
      "0.00030685213\n",
      "0.00029577082\n",
      "0.0002871023\n",
      "0.0002778684\n",
      "0.0002697698\n",
      "0.00026199163\n",
      "0.00025377033\n",
      "0.00024587286\n",
      "0.00023861049\n",
      "0.00023188029\n",
      "0.00022524707\n",
      "0.00021966512\n",
      "0.00021345833\n",
      "0.0002072311\n",
      "0.00020125354\n",
      "0.00019546505\n",
      "0.00019040435\n",
      "0.00018514576\n",
      "0.00018081156\n",
      "0.000175061\n",
      "0.00017001528\n",
      "0.0001662118\n",
      "0.00016190825\n",
      "0.00015719776\n",
      "0.000153398\n",
      "0.00014975281\n",
      "0.00014641945\n",
      "0.00014350377\n",
      "0.00013939402\n",
      "0.00013579565\n",
      "0.0001327222\n",
      "0.00012950464\n",
      "0.00012631962\n",
      "0.00012339166\n",
      "0.00012060713\n",
      "0.00011747689\n",
      "0.00011501833\n",
      "0.000112238966\n",
      "0.000110252564\n",
      "0.00010734424\n",
      "0.00010513013\n",
      "0.00010275344\n",
      "0.00010043211\n",
      "9.7922384e-05\n",
      "9.614986e-05\n",
      "9.4303614e-05\n",
      "9.252923e-05\n",
      "9.04637e-05\n",
      "8.829433e-05\n",
      "8.6501124e-05\n",
      "8.475061e-05\n",
      "8.297705e-05\n",
      "8.167632e-05\n",
      "8.0435275e-05\n",
      "7.862234e-05\n",
      "7.718137e-05\n",
      "7.607322e-05\n",
      "7.410939e-05\n",
      "7.23164e-05\n",
      "7.082232e-05\n",
      "6.978567e-05\n",
      "6.841091e-05\n",
      "6.7213216e-05\n",
      "6.595302e-05\n",
      "6.471437e-05\n",
      "6.351477e-05\n",
      "6.253089e-05\n",
      "6.1285784e-05\n",
      "6.0443974e-05\n",
      "5.9504924e-05\n",
      "5.851085e-05\n",
      "5.7518133e-05\n",
      "5.659841e-05\n",
      "5.5713394e-05\n",
      "5.449728e-05\n",
      "5.3609416e-05\n",
      "5.286724e-05\n",
      "5.2115796e-05\n",
      "5.1245548e-05\n",
      "5.0709692e-05\n",
      "4.9883365e-05\n",
      "4.916014e-05\n",
      "4.821815e-05\n",
      "4.7473273e-05\n",
      "4.6525503e-05\n",
      "4.6184272e-05\n",
      "4.5260076e-05\n",
      "4.4535074e-05\n",
      "4.386447e-05\n",
      "4.3368982e-05\n",
      "4.262371e-05\n",
      "4.183196e-05\n",
      "4.1430583e-05\n",
      "4.0836756e-05\n",
      "4.0157833e-05\n",
      "3.984496e-05\n",
      "3.9190716e-05\n",
      "3.8794653e-05\n",
      "3.832225e-05\n",
      "3.7868052e-05\n",
      "3.729653e-05\n",
      "3.6828613e-05\n",
      "3.6437203e-05\n",
      "3.6010617e-05\n",
      "3.564847e-05\n",
      "3.5111625e-05\n",
      "3.452834e-05\n",
      "3.416742e-05\n",
      "3.3784017e-05\n",
      "3.362744e-05\n",
      "3.3196877e-05\n",
      "3.273711e-05\n",
      "3.2461692e-05\n",
      "3.2040545e-05\n",
      "3.179575e-05\n",
      "3.146911e-05\n",
      "3.101962e-05\n",
      "3.055216e-05\n",
      "3.008524e-05\n",
      "2.9816583e-05\n",
      "2.9355537e-05\n",
      "2.8921098e-05\n",
      "2.869492e-05\n",
      "2.8343373e-05\n",
      "2.8024042e-05\n",
      "2.7764707e-05\n",
      "2.7396196e-05\n",
      "2.6981332e-05\n",
      "2.6715572e-05\n",
      "2.6444366e-05\n",
      "2.6364416e-05\n",
      "2.6084708e-05\n",
      "2.5913065e-05\n",
      "2.5536589e-05\n",
      "2.5350822e-05\n",
      "2.5301155e-05\n",
      "2.5066613e-05\n",
      "2.4698162e-05\n",
      "2.4319894e-05\n",
      "2.4166458e-05\n",
      "2.394945e-05\n",
      "2.3655124e-05\n",
      "2.3294497e-05\n",
      "2.3197645e-05\n",
      "2.2999848e-05\n",
      "2.2662545e-05\n",
      "2.266183e-05\n",
      "2.2486889e-05\n",
      "2.228375e-05\n",
      "2.2126458e-05\n",
      "2.1967631e-05\n",
      "2.1572754e-05\n",
      "2.1353047e-05\n",
      "2.126088e-05\n",
      "2.1120148e-05\n",
      "2.1005275e-05\n",
      "2.0807382e-05\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, D_in))\n",
    "y = tf.placeholder(tf.float32, shape=(None, D_out))\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal((D_in, H)))\n",
    "w2 = tf.Variable(tf.random_normal((H, D_out)))\n",
    "\n",
    "# Forward pass: \n",
    "h = tf.matmul(x, w1)\n",
    "h_relu = tf.maximum(h, tf.zeros(1))\n",
    "y_pred = tf.matmul(h_relu, w2)\n",
    "\n",
    "# Compute loss using operations on TensorFlow Tensors\n",
    "loss = tf.reduce_sum((y - y_pred) ** 2.0)\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
    "\n",
    "learning_rate = 1e-6\n",
    "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run the graph once to initialize the Variables w1 and w2.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    x_value = np.random.randn(N, D_in)\n",
    "    y_value = np.random.randn(N, D_out)\n",
    "    for _ in range(500):\n",
    "        loss_value, _, _ = sess.run([loss, new_w1, new_w2],\n",
    "                                    feed_dict={x: x_value, y: y_value})\n",
    "        print(loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-4\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "    if i % 100 == 0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss is 652.0419921875\n",
      "iter 100, loss is 42.43083572387695\n",
      "iter 200, loss is 0.48371249437332153\n",
      "iter 300, loss is 0.004649391397833824\n",
      "iter 400, loss is 3.5190525522921234e-05\n",
      "iter 500, loss is 9.032595471580862e-08\n",
      "iter 600, loss is 2.0706057701769254e-10\n",
      "iter 700, loss is 2.902242508817654e-11\n",
      "iter 800, loss is 1.0686113452451629e-11\n",
      "iter 900, loss is 5.718829489825827e-12\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    # model.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 100 == 0:\n",
    "        print(\"iter {}, loss is {}\".format(i, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss is 0.9272294044494629\n",
      "iter 500, loss is 0.7884258031845093\n"
     ]
    }
   ],
   "source": [
    "class TwoLayer(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayer, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.middle = torch.nn.Linear(H, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        for i in range(np.random.randint(0, 3)):\n",
    "            h_relu = self.middle(h_relu).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "    \n",
    "model = TwoLayer(D_in, H, D_out)\n",
    "losses = torch.nn.MSELoss(reduction='mean')\n",
    "optimizers = torch.optim.SGD(model.parameters(), lr=1e-6, momentum=0.99)\n",
    "\n",
    "for i in range(1000):\n",
    "    y_pred = model(x)\n",
    "    loss = losses(y_pred, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizers.step()\n",
    "    if i % 500 == 0:\n",
    "        print(\"iter {}, loss is {}\".format(i, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention.ipynb        csv_log.csv            rnnAdd.ipynb\r\n",
      "DocumentTest.ipynb     imdb_lstm.ipynb        \u001b[34mstock163\u001b[m\u001b[m/\r\n",
      "ISOlationForest.ipynb  keras.ipynb            testSlicedRNN.ipynb\r\n",
      "KerasAIPlay.ipynb      kerasDoc1.ipynb        tf_4dim.h5\r\n",
      "Rmr.ipynb              keras_convd1.h5        try1.txt\r\n",
      "SlicedRNN.ipynb        \u001b[34mlogs\u001b[m\u001b[m/                  \u001b[31myelp_2013.csv\u001b[m\u001b[m*\r\n",
      "TorchDnn.ipynb         no4.ipynb\r\n",
      "auto_model.hdf5        pytorchtest.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.utils' has no attribute 'checkpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-3f049b83ab59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_sequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.utils' has no attribute 'checkpoint'"
     ]
    }
   ],
   "source": [
    "torch.utils.checkpoint.checkpoint_sequential(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = torch.nn.Conv1d(16, 33, 3, stride=2)\n",
    "inputs = torch.randn(20, 16, 50)\n",
    "output = m(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 33, 24])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2,2],[2,3,5]],dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 2.],\n",
       "        [2., 3., 5.]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = torch.nn.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
