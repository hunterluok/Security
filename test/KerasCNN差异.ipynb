{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import  sequence\n",
    "from keras.datasets import  imdb\n",
    "from keras.models import  Sequential\n",
    "from keras.layers import  Embedding\n",
    "from keras.layers import LSTM, Dense, GRU\n",
    "from keras.utils import print_summary\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 80\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) = imdb.load_data(num_words=max_features)\n",
    "np.unique(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 526, 1: 474}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcute(data):\n",
    "    re ={}\n",
    "    for i in data:\n",
    "        re.setdefault(i,0)\n",
    "        re[i] +=1\n",
    "    return re\n",
    "calcute(y_test[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pad sequence\n",
    "x_train = sequence.pad_sequences(x_train,maxlen=maxlen, padding=\"post\")\n",
    "x_test = sequence.pad_sequences(x_test,maxlen=maxlen, padding=\"pre\") # 默认从前面截取和 填补"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features,128,input_length=maxlen))\n",
    "model.add(GRU(128,dropout=0.2,recurrent_dropout=0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train..\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      " 3872/25000 [===>..........................] - ETA: 1:11 - loss: 0.6634 - acc: 0.6090"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ef631ba291d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "print('train..')\n",
    "\n",
    "for i in range(2):\n",
    "    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=10, validation_data=(x_test, y_test))\n",
    "    plt.plot(history.history[\"acc\"])\n",
    "    plt.plot(history.history[\"val_acc\"])\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend([\"train\", \"test\"],loc=\"upper left\")\n",
    "    plt.show()\n",
    "    score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "    print(score, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convD1 一维卷积神经网络  对embeding不同处理的结果比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, Conv1D, GlobalMaxPooling1D, Activation, AveragePooling1D, MaxPooling1D\n",
    "from keras.layers import Input, Flatten\n",
    "from keras.models import  Model\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data ...\n",
      "(25000, 200) (25000, 200)\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen = 200\n",
    "batch_size = 128\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size =3\n",
    "hidden_dims = 250\n",
    "epochs = 10\n",
    "\n",
    "print(\"load data ...\")\n",
    "(train_x,train_y),(test_x,test_y) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "train_x = sequence.pad_sequences(train_x, maxlen=maxlen, padding=\"post\")\n",
    "test_x = sequence.pad_sequences(test_x, maxlen=maxlen, padding=\"post\")\n",
    "print(train_x.shape,test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178.0"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.median([len(i) for i in train_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import Conv2D, AveragePooling2D, MaxPooling2D,Subtract\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs_1 = Input(shape=(200,))\n",
    "x_embed = Embedding(input_dim=max_features, input_length=maxlen, \n",
    "                    output_dim=embedding_dims)(inputs_1)\n",
    "\n",
    "def expand_x(x):\n",
    "    x_embed = K.expand_dims(x, -1)\n",
    "    return x_embed\n",
    "\n",
    "x_embed = keras.layers.core.Lambda(expand_x)(x_embed)\n",
    "#x_embed = keras.layers.core.RepeatVector(1)(x_embed) 不可用， 这里的pading 注意。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_44 (InputLayer)        (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_47 (Embedding)     (None, 200, 50)           250000    \n",
      "_________________________________________________________________\n",
      "lambda_6 (Lambda)            (None, 200, 50, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 191, 1, 128)       64128     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 95, 1, 128)        0         \n",
      "_________________________________________________________________\n",
      "flatten_39 (Flatten)         (None, 12160)             0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 1)                 12161     \n",
      "=================================================================\n",
      "Total params: 326,289\n",
      "Trainable params: 326,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 38s 2ms/step - loss: 0.6845 - acc: 0.5412 - val_loss: 0.6261 - val_acc: 0.6423\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.64232, saving model to tf_4dim.h5\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 37s 1ms/step - loss: 0.5507 - acc: 0.7187 - val_loss: 0.6842 - val_acc: 0.6356\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.64232\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 37s 1ms/step - loss: 0.3774 - acc: 0.8356 - val_loss: 0.3254 - val_acc: 0.8632\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.64232 to 0.86316, saving model to tf_4dim.h5\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 36s 1ms/step - loss: 0.3082 - acc: 0.8712 - val_loss: 0.2991 - val_acc: 0.8730\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.86316 to 0.87300, saving model to tf_4dim.h5\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 35s 1ms/step - loss: 0.2680 - acc: 0.8890 - val_loss: 0.3031 - val_acc: 0.8700\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.87300\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 35s 1ms/step - loss: 0.2402 - acc: 0.9035 - val_loss: 0.2871 - val_acc: 0.8791\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.87300 to 0.87912, saving model to tf_4dim.h5\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 36s 1ms/step - loss: 0.2186 - acc: 0.9132 - val_loss: 0.2850 - val_acc: 0.8824\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.87912 to 0.88236, saving model to tf_4dim.h5\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 36s 1ms/step - loss: 0.2033 - acc: 0.9202 - val_loss: 0.3056 - val_acc: 0.8729\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.88236\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 36s 1ms/step - loss: 0.1826 - acc: 0.9295 - val_loss: 0.3047 - val_acc: 0.8786\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.88236\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 35s 1ms/step - loss: 0.1716 - acc: 0.9332 - val_loss: 0.3098 - val_acc: 0.8811\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.88236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13ef6b358>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_2 = Conv2D(128, kernel_size=(10, embedding_dims),data_format=\"channels_last\",\n",
    "               strides=(1, 1), padding=\"valid\", activation=\"relu\")(x_embed)\n",
    "con_2_pool = MaxPooling2D(pool_size=(2, 1))(con_2)\n",
    "\n",
    "con_2_pool_flat = Flatten()(con_2_pool)\n",
    "output_2 = Dense(1, activation=\"sigmoid\")(con_2_pool_flat)\n",
    "print(output_2.shape)\n",
    "\n",
    "model_1 = Model(inputs_1, output_2)\n",
    "model_1.summary()\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "savebestmodel = 'tf_4dim.h5'\n",
    "checkpoint = ModelCheckpoint(savebestmodel, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks=[checkpoint] \n",
    "\n",
    "model_1.compile(loss=\"binary_crossentropy\", optimizer=\"Adadelta\", metrics=[\"acc\"])\n",
    "model_1.fit(train_x, train_y, batch_size=batch_size, epochs=epochs, validation_data=(test_x, test_y),\n",
    "            verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.99492,\n",
       "  0.99848,\n",
       "  0.99732,\n",
       "  0.998,\n",
       "  0.998720000038147,\n",
       "  0.99956,\n",
       "  0.99964,\n",
       "  0.9996,\n",
       "  0.9976,\n",
       "  0.9998],\n",
       " 'loss': [0.021607562263011932,\n",
       "  0.012184968919456005,\n",
       "  0.012973040844500065,\n",
       "  0.011352410731464625,\n",
       "  0.0074836928868293765,\n",
       "  0.0054534724199771884,\n",
       "  0.004550972958952189,\n",
       "  0.004116294031441212,\n",
       "  0.00977890340629965,\n",
       "  0.002959204211831093],\n",
       " 'val_acc': [0.87468,\n",
       "  0.87424,\n",
       "  0.87224,\n",
       "  0.873,\n",
       "  0.86952,\n",
       "  0.872639999961853,\n",
       "  0.872719999961853,\n",
       "  0.87068,\n",
       "  0.871759999961853,\n",
       "  0.86652],\n",
       " 'val_loss': [0.6045735818386078,\n",
       "  0.6432584668159484,\n",
       "  0.6620480927181244,\n",
       "  0.6754186646938324,\n",
       "  0.7101701876068115,\n",
       "  0.7297660604286194,\n",
       "  0.7535416314125061,\n",
       "  0.7661286487960816,\n",
       "  0.753690506286621,\n",
       "  0.8445094299507141]}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_45 (InputLayer)        (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_48 (Embedding)     (None, 200, 50)           250000    \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 191, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 95, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_40 (Flatten)         (None, 12160)             0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 1)                 12161     \n",
      "=================================================================\n",
      "Total params: 326,289\n",
      "Trainable params: 326,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 30s 1ms/step - loss: 0.6648 - acc: 0.5798 - val_loss: 0.5533 - val_acc: 0.7253\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.72532, saving model to keras_convd1.h5\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 29s 1ms/step - loss: 0.4773 - acc: 0.7746 - val_loss: 0.5390 - val_acc: 0.7418\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.72532 to 0.74180, saving model to keras_convd1.h5\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 29s 1ms/step - loss: 0.3506 - acc: 0.8500 - val_loss: 0.3072 - val_acc: 0.8683\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.74180 to 0.86828, saving model to keras_convd1.h5\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 29s 1ms/step - loss: 0.2950 - acc: 0.8746 - val_loss: 0.3725 - val_acc: 0.8368\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.86828\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 29s 1ms/step - loss: 0.2552 - acc: 0.8969 - val_loss: 0.2867 - val_acc: 0.8785\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.86828 to 0.87848, saving model to keras_convd1.h5\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 29s 1ms/step - loss: 0.2299 - acc: 0.9072 - val_loss: 0.2797 - val_acc: 0.8826\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.87848 to 0.88260, saving model to keras_convd1.h5\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 30s 1ms/step - loss: 0.2049 - acc: 0.9200 - val_loss: 0.3061 - val_acc: 0.8749\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.88260\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 29s 1ms/step - loss: 0.1848 - acc: 0.9292 - val_loss: 0.2901 - val_acc: 0.8834\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.88260 to 0.88336, saving model to keras_convd1.h5\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 30s 1ms/step - loss: 0.1661 - acc: 0.9365 - val_loss: 0.2931 - val_acc: 0.8821\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.88336\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 31s 1ms/step - loss: 0.1472 - acc: 0.9462 - val_loss: 0.3724 - val_acc: 0.8602\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.88336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x163ec9668>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_2 = Input(shape=(200,))\n",
    "x_embed_1 = Embedding(input_dim=max_features, input_length=maxlen, output_dim=embedding_dims)(inputs_2)\n",
    "\n",
    "x_convd_1 = Conv1D(128, kernel_size=10, activation=\"relu\")(x_embed_1)\n",
    "x_convd_1_pool = MaxPooling1D()(x_convd_1)\n",
    "\n",
    "con_1_pool_flat = Flatten()(x_convd_1_pool)\n",
    "output_1 = Dense(1, activation=\"sigmoid\")(con_1_pool_flat)\n",
    "print(output_1.shape)\n",
    "\n",
    "model_com = Model(inputs_2, output_1)\n",
    "model_com.summary()\n",
    "           \n",
    "savebestmodel = 'keras_convd1.h5'\n",
    "checkpoint = ModelCheckpoint(savebestmodel, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_com=[checkpoint] \n",
    "\n",
    "model_com.compile(loss=\"binary_crossentropy\", optimizer=\"Adadelta\", metrics=[\"acc\"])\n",
    "model_com.fit(train_x, train_y, batch_size=batch_size, epochs=epochs, validation_data=(test_x, test_y),\n",
    "            verbose=1, callbacks=callbacks_com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cnn 多个 卷积组合的搞法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv1D 类似与 4维中 kerner_size = (N, embedding_dims)(())\n",
    "# (2,)*1 + (1, )\n",
    "# (1, 2) + (2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 78, 128)\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(400,))\n",
    "x_embed = Embedding(input_dim=max_features, output_dim=embedding_dims,input_length=maxlen)(inputs)\n",
    "x_conved_1 = Conv1D(128, kernel_size=10, strides=5, padding=\"valid\", activation=\"relu\")(x_embed)\n",
    "x_conved_1_drop = Dropout(0.2)(x_conved_1)\n",
    "x_1_pool = AveragePooling1D(pool_size=2)(x_conved_1_drop)\n",
    "\n",
    "x_conved_2 = Conv1D(128, kernel_size=15, strides=5, padding=\"valid\", activation=\"relu\")(x_embed)\n",
    "x_conved_2_drop = Dropout(0.2)(x_conved_2)\n",
    "x_2_pool = AveragePooling1D(pool_size=2)(x_conved_2_drop)\n",
    "print(x_conved_2.shape)\n",
    "\n",
    "x_conved_3 = Conv1D(128, kernel_size=20, strides=5, padding=\"valid\", activation=\"relu\")(x_embed)\n",
    "x_conved_3_drop = Dropout(0.2)(x_conved_3)\n",
    "x_3_pool = AveragePooling1D(pool_size=2)(x_conved_3_drop)\n",
    "# pool是 向下取整用的\n",
    "\n",
    "megerd = keras.layers.concatenate([x_1_pool, x_2_pool, x_3_pool], axis=1)\n",
    "megerd = Flatten()(megerd)\n",
    "\n",
    "output = Dense(1, activation=\"sigmoid\")(megerd)\n",
    "\n",
    "model = Model(inputs, output)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 62s 2ms/step - loss: 0.5379 - acc: 0.6988 - val_loss: 0.3582 - val_acc: 0.8400\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 63s 3ms/step - loss: 0.2879 - acc: 0.8788 - val_loss: 0.2679 - val_acc: 0.8861\n",
      "25000/25000 [==============================] - 12s 463us/step\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"Adadelta\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "model.fit(train_x, train_y, batch_size=batch_size, epochs=epochs,\n",
    "          validation_data=(test_x,test_y), verbose=1)\n",
    "# model.fit(x_train,y_train,batch_size=batch_size,validation_data=(x_test,y_test),epochs=2)\n",
    "# score, acc = model.evaluate(test_x, test_y, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 400, 50)           250000    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 400, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 398, 250)          37750     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 350,751\n",
      "Trainable params: 350,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dims = 50\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = max_features,output_dim=embedding_dims,input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(filters=filters,kernel_size=kernel_size,padding='valid',activation='relu',strides=1))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "# model.compile(optimizer=\"Adadelta\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "# model.fit(x_train,y_train,batch_size=batch_size,validation_data=(x_test,y_test),epochs=2)\n",
    "# score,acc = model.evaluate(test_x,test_y,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2741204485177994 0.88688\n"
     ]
    }
   ],
   "source": [
    "print(score,acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTO_ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "   57344/11490434 [..............................] - ETA: 16:18:09"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    " \n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model #泛型模型\n",
    "from keras.layers import Dense, Input\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# X shape (60,000 28x28), y shape (10,000, )\n",
    "(x_train, _), (x_test, y_test) = mnist.load_data()\n",
    " \n",
    "# 数据预处理\n",
    "x_train = x_train.astype('float32') / 255. - 0.5       # minmax_normalized\n",
    "x_test = x_test.astype('float32') / 255. - 0.5         # minmax_normalized\n",
    "x_train = x_train.reshape((x_train.shape[0], -1))\n",
    "x_test = x_test.reshape((x_test.shape[0], -1))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 压缩特征维度至2维\n",
    "encoding_dim = 2\n",
    " \n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(784,))\n",
    " \n",
    "# 编码层\n",
    "encoded = Dense(128, activation='relu')(input_img)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "encoded = Dense(10, activation='relu')(encoded)\n",
    "encoder_output = Dense(encoding_dim)(encoded)\n",
    " \n",
    "# 解码层\n",
    "decoded = Dense(10, activation='relu')(encoder_output)\n",
    "decoded = Dense(64, activation='relu')(decoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "decoded = Dense(784, activation='tanh')(decoded)\n",
    " \n",
    "# 构建自编码模型\n",
    "autoencoder = Model(inputs=input_img, outputs=decoded)\n",
    " \n",
    "# 构建编码模型\n",
    "encoder = Model(inputs=input_img, outputs=encoder_output)\n",
    " \n",
    "# compile autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    " \n",
    "# training\n",
    "autoencoder.fit(x_train, x_train, epochs=20, batch_size=256, shuffle=True)\n",
    " \n",
    "# plotting\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "plt.scatter(encoded_imgs[:, 0], encoded_imgs[:, 1], c=y_test, s=3)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
